{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b490a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91091328",
   "metadata": {},
   "outputs": [],
   "source": [
    "##function to read files and return string\n",
    "def read_file(filepath):\n",
    "    \n",
    "    with open(filepath,  encoding='utf8') as file:\n",
    "        string = file.read()\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df55010",
   "metadata": {},
   "outputs": [],
   "source": [
    "##reading .txt file\n",
    "string = read_file('psychology_of_money.txt') ##use entire filepath if file not in same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8adc5084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhin\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.6.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "##loading the language model\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39fe89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##function to remove unnecessary punctuations\n",
    "def filter_punc(doc):\n",
    "    return [token.text.lower() for token in nlp(doc) if token.text not in '\\n\\x0c \\n\\x0c\\x0c \\x0c\\x0c\\x0c               \\n\\n \\n\\n   \\n\\n\\n\\n \\n\\n\\n\\n\\n \\x0c\\x0c\\x0c \\x0c \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d5b72c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##getting tokens\n",
    "tokens = filter_punc(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2468916c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55620"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##generating sequences from the text\n",
    "\n",
    "train_len = 15 + 1 ##take 15 words as input to generate the 16th word\n",
    "\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):\n",
    "    sequence = ' '.join(tokens[i-train_len:i]) ##generate train_len worded text sequence\n",
    "    text_sequences.append(sequence)\n",
    "    \n",
    "len(text_sequences) ##len(tokens) - train_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e94d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##check first three sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7bcf6a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for my parents who teach me gretchen who guides me miles and reese who inspire me'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b460f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my parents who teach me gretchen who guides me miles and reese who inspire me introduction'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b54d3b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parents who teach me gretchen who guides me miles and reese who inspire me introduction the'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d6621cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TOKENIZATION\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer() ##instantiation\n",
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f7c76b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "##function to tokenize each string\n",
    "def tokenize(string, tokenizer, train_len):\n",
    "    \n",
    "    encoded_sequence = tokenizer.texts_to_sequences([string])[0]\n",
    "    padded_sequence = pad_sequences([encoded_sequence], maxlen=train_len, padding='pre', truncating='pre')[0]\n",
    "    \n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "46c667ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "##getting encoded sequences\n",
    "sequences = np.array([tokenize(string=sequence, tokenizer=tokenizer, train_len=train_len) for sequence in text_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20bde05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  12,   79,  516, ...,   56, 6187,  111],\n",
       "       [  79,  516,   56, ..., 6187,  111, 2271],\n",
       "       [ 516,   56, 1061, ...,  111, 2271,    1],\n",
       "       ...,\n",
       "       [   6,   39,  182, ...,   41,    1, 6189],\n",
       "       [  39,  182,   34, ...,    1, 6189,    2],\n",
       "       [ 182,   34,   23, ..., 6189,    2,    1]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd611adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##feature label split\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X, y = sequences[:,:-1], sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1aec168a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  12,   79,  516, ..., 6188,   56, 6187],\n",
       "       [  79,  516,   56, ...,   56, 6187,  111],\n",
       "       [ 516,   56, 1061, ..., 6187,  111, 2271],\n",
       "       ...,\n",
       "       [   6,   39,  182, ...,   36,   41,    1],\n",
       "       [  39,  182,   34, ...,   41,    1, 6189],\n",
       "       [ 182,   34,   23, ...,    1, 6189,    2]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a0efb111",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X.shape[1] ##input length to the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "79885777",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(tokenizer.index_word) ##length of tokenizer vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b2cfb279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_len+1) ##keras needs one additional column for padding\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1b3799b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##MODEL BUILDING\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "14142935",
   "metadata": {},
   "outputs": [],
   "source": [
    "##function to create model\n",
    "def create_model(vocab, seq_len):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(vocab, 15, input_length=seq_len))\n",
    "    \n",
    "    model.add(LSTM(units=10*seq_len, return_sequences=True))\n",
    "    model.add(LSTM(units=10*seq_len))\n",
    "    model.add(Dense(units=10*seq_len, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(units=vocab, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "30b10f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##importing dump and load to save model and tokenizer\n",
    "from pickle import dump, load\n",
    "\n",
    "dump(tokenizer, open('tokenizer','wb')) ##save the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "69a4c76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 15, 15)            92850     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 15, 150)           99600     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 150)               180600    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 150)               22650     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6190)              934690    \n",
      "=================================================================\n",
      "Total params: 1,330,390\n",
      "Trainable params: 1,330,390\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##create model\n",
    "model = create_model(vocab=vocab_len+1, seq_len=seq_len) ##keras needs one extra column for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a226d86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "55620/55620 [==============================] - 61s 1ms/step - loss: 6.9283 - accuracy: 0.0453\n",
      "Epoch 2/300\n",
      "55620/55620 [==============================] - 56s 1ms/step - loss: 6.5958 - accuracy: 0.0474\n",
      "Epoch 3/300\n",
      "55620/55620 [==============================] - 50s 893us/step - loss: 6.4207 - accuracy: 0.0585\n",
      "Epoch 4/300\n",
      "55620/55620 [==============================] - 49s 887us/step - loss: 6.2805 - accuracy: 0.0651\n",
      "Epoch 5/300\n",
      "55620/55620 [==============================] - 49s 885us/step - loss: 6.1715 - accuracy: 0.0700\n",
      "Epoch 6/300\n",
      "55620/55620 [==============================] - 50s 897us/step - loss: 6.0610 - accuracy: 0.0767\n",
      "Epoch 7/300\n",
      "55620/55620 [==============================] - 54s 962us/step - loss: 6.0224 - accuracy: 0.0780\n",
      "Epoch 8/300\n",
      "55620/55620 [==============================] - 59s 1ms/step - loss: 5.8718 - accuracy: 0.0929\n",
      "Epoch 9/300\n",
      "55620/55620 [==============================] - 59s 1ms/step - loss: 5.7579 - accuracy: 0.1007\n",
      "Epoch 10/300\n",
      "55620/55620 [==============================] - 55s 989us/step - loss: 5.6502 - accuracy: 0.1048\n",
      "Epoch 11/300\n",
      "55620/55620 [==============================] - 49s 882us/step - loss: 5.5460 - accuracy: 0.1084\n",
      "Epoch 12/300\n",
      "55620/55620 [==============================] - 48s 866us/step - loss: 5.4555 - accuracy: 0.1131\n",
      "Epoch 13/300\n",
      "55620/55620 [==============================] - 48s 862us/step - loss: 5.3559 - accuracy: 0.1171\n",
      "Epoch 14/300\n",
      "55620/55620 [==============================] - 48s 864us/step - loss: 5.2541 - accuracy: 0.1185\n",
      "Epoch 15/300\n",
      "55620/55620 [==============================] - 48s 871us/step - loss: 5.1642 - accuracy: 0.1204\n",
      "Epoch 16/300\n",
      "55620/55620 [==============================] - 48s 864us/step - loss: 5.0496 - accuracy: 0.1243\n",
      "Epoch 17/300\n",
      "55620/55620 [==============================] - 48s 865us/step - loss: 4.9912 - accuracy: 0.1256\n",
      "Epoch 18/300\n",
      "55620/55620 [==============================] - 48s 864us/step - loss: 4.9253 - accuracy: 0.1271\n",
      "Epoch 19/300\n",
      "55620/55620 [==============================] - 48s 866us/step - loss: 4.8178 - accuracy: 0.1300\n",
      "Epoch 20/300\n",
      "55620/55620 [==============================] - 48s 870us/step - loss: 4.7328 - accuracy: 0.1322\n",
      "Epoch 21/300\n",
      "55620/55620 [==============================] - 48s 867us/step - loss: 4.6477 - accuracy: 0.1374\n",
      "Epoch 22/300\n",
      "55620/55620 [==============================] - 48s 867us/step - loss: 4.5584 - accuracy: 0.1408\n",
      "Epoch 23/300\n",
      "55620/55620 [==============================] - 48s 868us/step - loss: 4.5242 - accuracy: 0.1415\n",
      "Epoch 24/300\n",
      "55620/55620 [==============================] - 48s 870us/step - loss: 4.5112 - accuracy: 0.1442\n",
      "Epoch 25/300\n",
      "55620/55620 [==============================] - 48s 871us/step - loss: 4.4766 - accuracy: 0.1475\n",
      "Epoch 26/300\n",
      "55620/55620 [==============================] - 48s 866us/step - loss: 4.5659 - accuracy: 0.1413\n",
      "Epoch 27/300\n",
      "55620/55620 [==============================] - 49s 872us/step - loss: 4.4822 - accuracy: 0.1446\n",
      "Epoch 28/300\n",
      "55620/55620 [==============================] - 48s 870us/step - loss: 4.3825 - accuracy: 0.1512\n",
      "Epoch 29/300\n",
      "55620/55620 [==============================] - 49s 874us/step - loss: 4.2997 - accuracy: 0.1577\n",
      "Epoch 30/300\n",
      "55620/55620 [==============================] - 49s 878us/step - loss: 4.2208 - accuracy: 0.1653\n",
      "Epoch 31/300\n",
      "55620/55620 [==============================] - 49s 873us/step - loss: 4.1504 - accuracy: 0.1701\n",
      "Epoch 32/300\n",
      "55620/55620 [==============================] - 48s 870us/step - loss: 4.0864 - accuracy: 0.1782\n",
      "Epoch 33/300\n",
      "55620/55620 [==============================] - 49s 875us/step - loss: 4.0217 - accuracy: 0.1850\n",
      "Epoch 34/300\n",
      "55620/55620 [==============================] - 48s 872us/step - loss: 3.9591 - accuracy: 0.1916\n",
      "Epoch 35/300\n",
      "55620/55620 [==============================] - 48s 871us/step - loss: 3.9056 - accuracy: 0.1989\n",
      "Epoch 36/300\n",
      "55620/55620 [==============================] - 49s 875us/step - loss: 3.8480 - accuracy: 0.2049\n",
      "Epoch 37/300\n",
      "55620/55620 [==============================] - 49s 873us/step - loss: 3.7965 - accuracy: 0.2107\n",
      "Epoch 38/300\n",
      "55620/55620 [==============================] - 48s 870us/step - loss: 3.7460 - accuracy: 0.2177\n",
      "Epoch 39/300\n",
      "55620/55620 [==============================] - 52s 928us/step - loss: 3.7015 - accuracy: 0.2230\n",
      "Epoch 40/300\n",
      "55620/55620 [==============================] - 49s 881us/step - loss: 3.6522 - accuracy: 0.2315\n",
      "Epoch 41/300\n",
      "55620/55620 [==============================] - 48s 871us/step - loss: 3.6078 - accuracy: 0.2358\n",
      "Epoch 42/300\n",
      "55620/55620 [==============================] - 49s 873us/step - loss: 3.5638 - accuracy: 0.2422\n",
      "Epoch 43/300\n",
      "55620/55620 [==============================] - 48s 870us/step - loss: 3.5238 - accuracy: 0.2491\n",
      "Epoch 44/300\n",
      "55620/55620 [==============================] - 48s 871us/step - loss: 3.4809 - accuracy: 0.2546\n",
      "Epoch 45/300\n",
      "55620/55620 [==============================] - 49s 875us/step - loss: 3.4440 - accuracy: 0.2601\n",
      "Epoch 46/300\n",
      "55620/55620 [==============================] - 48s 860us/step - loss: 3.4041 - accuracy: 0.2664\n",
      "Epoch 47/300\n",
      "55620/55620 [==============================] - 48s 856us/step - loss: 3.3678 - accuracy: 0.2722\n",
      "Epoch 48/300\n",
      "55620/55620 [==============================] - 48s 854us/step - loss: 3.3277 - accuracy: 0.2784\n",
      "Epoch 49/300\n",
      "55620/55620 [==============================] - 48s 859us/step - loss: 3.2912 - accuracy: 0.2833\n",
      "Epoch 50/300\n",
      "55620/55620 [==============================] - 48s 856us/step - loss: 3.2604 - accuracy: 0.2879\n",
      "Epoch 51/300\n",
      "55620/55620 [==============================] - 48s 860us/step - loss: 3.2285 - accuracy: 0.2920\n",
      "Epoch 52/300\n",
      "55620/55620 [==============================] - 48s 872us/step - loss: 3.1920 - accuracy: 0.2984\n",
      "Epoch 53/300\n",
      "55620/55620 [==============================] - 48s 862us/step - loss: 3.1591 - accuracy: 0.3037\n",
      "Epoch 54/300\n",
      "55620/55620 [==============================] - 48s 860us/step - loss: 3.1293 - accuracy: 0.3072\n",
      "Epoch 55/300\n",
      "55620/55620 [==============================] - 48s 857us/step - loss: 3.0948 - accuracy: 0.3152\n",
      "Epoch 56/300\n",
      "55620/55620 [==============================] - 48s 855us/step - loss: 3.0667 - accuracy: 0.3198\n",
      "Epoch 57/300\n",
      "55620/55620 [==============================] - 48s 856us/step - loss: 3.0313 - accuracy: 0.3242\n",
      "Epoch 58/300\n",
      "55620/55620 [==============================] - 48s 859us/step - loss: 2.9989 - accuracy: 0.3292\n",
      "Epoch 59/300\n",
      "55620/55620 [==============================] - 48s 860us/step - loss: 2.9723 - accuracy: 0.3337\n",
      "Epoch 60/300\n",
      "55620/55620 [==============================] - 48s 857us/step - loss: 2.9411 - accuracy: 0.3377\n",
      "Epoch 61/300\n",
      "55620/55620 [==============================] - 48s 855us/step - loss: 2.9123 - accuracy: 0.3462\n",
      "Epoch 62/300\n",
      "55620/55620 [==============================] - 48s 857us/step - loss: 2.8843 - accuracy: 0.3482\n",
      "Epoch 63/300\n",
      "55620/55620 [==============================] - 48s 863us/step - loss: 2.8558 - accuracy: 0.3537\n",
      "Epoch 64/300\n",
      "55620/55620 [==============================] - 48s 860us/step - loss: 2.8250 - accuracy: 0.3581\n",
      "Epoch 65/300\n",
      "55620/55620 [==============================] - 48s 857us/step - loss: 2.7982 - accuracy: 0.3632\n",
      "Epoch 66/300\n",
      "55620/55620 [==============================] - 48s 871us/step - loss: 2.7712 - accuracy: 0.3672\n",
      "Epoch 67/300\n",
      "55620/55620 [==============================] - 48s 859us/step - loss: 2.7387 - accuracy: 0.3754\n",
      "Epoch 68/300\n",
      "55620/55620 [==============================] - 48s 859us/step - loss: 2.7178 - accuracy: 0.3790\n",
      "Epoch 69/300\n",
      "55620/55620 [==============================] - 49s 875us/step - loss: 2.6890 - accuracy: 0.3831\n",
      "Epoch 70/300\n",
      "55620/55620 [==============================] - 48s 871us/step - loss: 2.6642 - accuracy: 0.3871\n",
      "Epoch 71/300\n",
      "55620/55620 [==============================] - 48s 862us/step - loss: 2.6325 - accuracy: 0.3938\n",
      "Epoch 72/300\n",
      "55620/55620 [==============================] - 48s 862us/step - loss: 2.6100 - accuracy: 0.3967\n",
      "Epoch 73/300\n",
      "55620/55620 [==============================] - 48s 855us/step - loss: 2.5806 - accuracy: 0.4028\n",
      "Epoch 74/300\n",
      "55620/55620 [==============================] - 48s 857us/step - loss: 2.5578 - accuracy: 0.4079\n",
      "Epoch 75/300\n",
      "55620/55620 [==============================] - 48s 867us/step - loss: 2.5341 - accuracy: 0.4099\n",
      "Epoch 76/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55620/55620 [==============================] - 47s 849us/step - loss: 2.5065 - accuracy: 0.4153\n",
      "Epoch 77/300\n",
      "55620/55620 [==============================] - 48s 858us/step - loss: 2.4787 - accuracy: 0.4216\n",
      "Epoch 78/300\n",
      "55620/55620 [==============================] - 47s 847us/step - loss: 2.4614 - accuracy: 0.4233\n",
      "Epoch 79/300\n",
      "55620/55620 [==============================] - 47s 848us/step - loss: 2.4281 - accuracy: 0.4314\n",
      "Epoch 80/300\n",
      "55620/55620 [==============================] - 47s 848us/step - loss: 2.4071 - accuracy: 0.4371\n",
      "Epoch 81/300\n",
      "55620/55620 [==============================] - ETA: 0s - loss: 2.3819 - accuracy: 0.43 - 47s 853us/step - loss: 2.3822 - accuracy: 0.4392\n",
      "Epoch 82/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 2.3581 - accuracy: 0.4439\n",
      "Epoch 83/300\n",
      "55620/55620 [==============================] - 47s 851us/step - loss: 2.3350 - accuracy: 0.4469\n",
      "Epoch 84/300\n",
      "55620/55620 [==============================] - 47s 854us/step - loss: 2.3132 - accuracy: 0.4533\n",
      "Epoch 85/300\n",
      "55620/55620 [==============================] - 48s 865us/step - loss: 2.2870 - accuracy: 0.4587\n",
      "Epoch 86/300\n",
      "55620/55620 [==============================] - 47s 850us/step - loss: 2.2660 - accuracy: 0.4623\n",
      "Epoch 87/300\n",
      "55620/55620 [==============================] - 48s 856us/step - loss: 2.2404 - accuracy: 0.4665\n",
      "Epoch 88/300\n",
      "55620/55620 [==============================] - 47s 850us/step - loss: 2.2216 - accuracy: 0.4713\n",
      "Epoch 89/300\n",
      "55620/55620 [==============================] - 48s 858us/step - loss: 2.1943 - accuracy: 0.4768\n",
      "Epoch 90/300\n",
      "55620/55620 [==============================] - 48s 858us/step - loss: 2.1764 - accuracy: 0.4817\n",
      "Epoch 91/300\n",
      "55620/55620 [==============================] - 47s 850us/step - loss: 2.1496 - accuracy: 0.4851\n",
      "Epoch 92/300\n",
      "55620/55620 [==============================] - 47s 854us/step - loss: 2.1289 - accuracy: 0.4910\n",
      "Epoch 93/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 2.1064 - accuracy: 0.4950\n",
      "Epoch 94/300\n",
      "55620/55620 [==============================] - 47s 853us/step - loss: 2.0902 - accuracy: 0.4970\n",
      "Epoch 95/300\n",
      "55620/55620 [==============================] - 47s 851us/step - loss: 2.0640 - accuracy: 0.5021\n",
      "Epoch 96/300\n",
      "55620/55620 [==============================] - 47s 854us/step - loss: 2.0396 - accuracy: 0.5096\n",
      "Epoch 97/300\n",
      "55620/55620 [==============================] - 47s 853us/step - loss: 2.0218 - accuracy: 0.5123\n",
      "Epoch 98/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 1.9952 - accuracy: 0.5195\n",
      "Epoch 99/300\n",
      "55620/55620 [==============================] - 47s 853us/step - loss: 1.9805 - accuracy: 0.5209\n",
      "Epoch 100/300\n",
      "55620/55620 [==============================] - 48s 861us/step - loss: 1.9570 - accuracy: 0.5275\n",
      "Epoch 101/300\n",
      "55620/55620 [==============================] - 47s 854us/step - loss: 1.9355 - accuracy: 0.5297\n",
      "Epoch 102/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 1.9282 - accuracy: 0.5307\n",
      "Epoch 103/300\n",
      "55620/55620 [==============================] - 48s 856us/step - loss: 1.9158 - accuracy: 0.5324\n",
      "Epoch 104/300\n",
      "55620/55620 [==============================] - 48s 858us/step - loss: 1.8787 - accuracy: 0.5420\n",
      "Epoch 105/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 1.8535 - accuracy: 0.5474\n",
      "Epoch 106/300\n",
      "55620/55620 [==============================] - 47s 851us/step - loss: 1.8334 - accuracy: 0.5528\n",
      "Epoch 107/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 1.8138 - accuracy: 0.5587\n",
      "Epoch 108/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 1.7944 - accuracy: 0.5612\n",
      "Epoch 109/300\n",
      "55620/55620 [==============================] - 48s 854us/step - loss: 1.7788 - accuracy: 0.5638\n",
      "Epoch 110/300\n",
      "55620/55620 [==============================] - 47s 851us/step - loss: 1.7590 - accuracy: 0.5679\n",
      "Epoch 111/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 1.7370 - accuracy: 0.5717\n",
      "Epoch 112/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 1.7214 - accuracy: 0.5767\n",
      "Epoch 113/300\n",
      "55620/55620 [==============================] - 47s 851us/step - loss: 1.7063 - accuracy: 0.5793\n",
      "Epoch 114/300\n",
      "55620/55620 [==============================] - 47s 851us/step - loss: 1.6820 - accuracy: 0.5850\n",
      "Epoch 115/300\n",
      "55620/55620 [==============================] - 48s 857us/step - loss: 1.6666 - accuracy: 0.5879\n",
      "Epoch 116/300\n",
      "55620/55620 [==============================] - 47s 851us/step - loss: 1.6469 - accuracy: 0.5924\n",
      "Epoch 117/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 1.6285 - accuracy: 0.5967\n",
      "Epoch 118/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 1.6120 - accuracy: 0.6016\n",
      "Epoch 119/300\n",
      "55620/55620 [==============================] - 48s 856us/step - loss: 1.5920 - accuracy: 0.6040\n",
      "Epoch 120/300\n",
      "55620/55620 [==============================] - 48s 855us/step - loss: 1.5720 - accuracy: 0.6115\n",
      "Epoch 121/300\n",
      "55620/55620 [==============================] - 48s 855us/step - loss: 1.5539 - accuracy: 0.6128\n",
      "Epoch 122/300\n",
      "55620/55620 [==============================] - 47s 853us/step - loss: 1.5415 - accuracy: 0.6150\n",
      "Epoch 123/300\n",
      "55620/55620 [==============================] - 47s 851us/step - loss: 1.5302 - accuracy: 0.6177\n",
      "Epoch 124/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 1.5046 - accuracy: 0.6232\n",
      "Epoch 125/300\n",
      "55620/55620 [==============================] - 48s 856us/step - loss: 1.4900 - accuracy: 0.6276\n",
      "Epoch 126/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 1.4725 - accuracy: 0.6321\n",
      "Epoch 127/300\n",
      "55620/55620 [==============================] - 48s 855us/step - loss: 1.4519 - accuracy: 0.6359\n",
      "Epoch 128/300\n",
      "55620/55620 [==============================] - 48s 855us/step - loss: 1.4354 - accuracy: 0.6394\n",
      "Epoch 129/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 1.4274 - accuracy: 0.6413\n",
      "Epoch 130/300\n",
      "55620/55620 [==============================] - 47s 854us/step - loss: 1.4200 - accuracy: 0.6424\n",
      "Epoch 131/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 1.3882 - accuracy: 0.6500\n",
      "Epoch 132/300\n",
      "55620/55620 [==============================] - 47s 853us/step - loss: 1.3692 - accuracy: 0.6559\n",
      "Epoch 133/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 1.3528 - accuracy: 0.6597\n",
      "Epoch 134/300\n",
      "55620/55620 [==============================] - 48s 871us/step - loss: 1.3427 - accuracy: 0.6616\n",
      "Epoch 135/300\n",
      "55620/55620 [==============================] - 48s 856us/step - loss: 1.3355 - accuracy: 0.6635\n",
      "Epoch 136/300\n",
      "55620/55620 [==============================] - 47s 847us/step - loss: 1.3160 - accuracy: 0.6648\n",
      "Epoch 137/300\n",
      "55620/55620 [==============================] - 47s 849us/step - loss: 1.2917 - accuracy: 0.6730\n",
      "Epoch 138/300\n",
      "55620/55620 [==============================] - 48s 858us/step - loss: 1.2872 - accuracy: 0.6729\n",
      "Epoch 139/300\n",
      "55620/55620 [==============================] - 47s 851us/step - loss: 1.2766 - accuracy: 0.6759\n",
      "Epoch 140/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 1.2586 - accuracy: 0.6800\n",
      "Epoch 141/300\n",
      "55620/55620 [==============================] - 47s 851us/step - loss: 1.2346 - accuracy: 0.6867\n",
      "Epoch 142/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 1.2179 - accuracy: 0.6910\n",
      "Epoch 143/300\n",
      "55620/55620 [==============================] - 47s 849us/step - loss: 1.2001 - accuracy: 0.6954\n",
      "Epoch 144/300\n",
      "55620/55620 [==============================] - 47s 847us/step - loss: 1.1817 - accuracy: 0.6992\n",
      "Epoch 145/300\n",
      "55620/55620 [==============================] - 47s 849us/step - loss: 1.1706 - accuracy: 0.7023\n",
      "Epoch 146/300\n",
      "55620/55620 [==============================] - 48s 858us/step - loss: 1.1643 - accuracy: 0.7024\n",
      "Epoch 147/300\n",
      "55620/55620 [==============================] - 48s 855us/step - loss: 1.1537 - accuracy: 0.7067\n",
      "Epoch 148/300\n",
      "55620/55620 [==============================] - 47s 848us/step - loss: 1.1326 - accuracy: 0.7095\n",
      "Epoch 149/300\n",
      "55620/55620 [==============================] - 47s 849us/step - loss: 1.1279 - accuracy: 0.7097\n",
      "Epoch 150/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55620/55620 [==============================] - 47s 843us/step - loss: 1.0999 - accuracy: 0.7188\n",
      "Epoch 151/300\n",
      "55620/55620 [==============================] - 47s 840us/step - loss: 1.0855 - accuracy: 0.7219\n",
      "Epoch 152/300\n",
      "55620/55620 [==============================] - 47s 842us/step - loss: 1.0894 - accuracy: 0.7201\n",
      "Epoch 153/300\n",
      "55620/55620 [==============================] - 47s 842us/step - loss: 1.0688 - accuracy: 0.7255\n",
      "Epoch 154/300\n",
      "55620/55620 [==============================] - 47s 840us/step - loss: 1.0589 - accuracy: 0.7294\n",
      "Epoch 155/300\n",
      "55620/55620 [==============================] - 47s 842us/step - loss: 1.0401 - accuracy: 0.7329\n",
      "Epoch 156/300\n",
      "55620/55620 [==============================] - 47s 844us/step - loss: 1.0111 - accuracy: 0.7427\n",
      "Epoch 157/300\n",
      "55620/55620 [==============================] - 47s 845us/step - loss: 1.0046 - accuracy: 0.7431\n",
      "Epoch 158/300\n",
      "55620/55620 [==============================] - 47s 848us/step - loss: 1.0143 - accuracy: 0.7368\n",
      "Epoch 159/300\n",
      "55620/55620 [==============================] - 47s 848us/step - loss: 0.9785 - accuracy: 0.7487\n",
      "Epoch 160/300\n",
      "55620/55620 [==============================] - 47s 843us/step - loss: 0.9762 - accuracy: 0.7475\n",
      "Epoch 161/300\n",
      "55620/55620 [==============================] - 47s 846us/step - loss: 0.9565 - accuracy: 0.7544\n",
      "Epoch 162/300\n",
      "55620/55620 [==============================] - 47s 840us/step - loss: 0.9503 - accuracy: 0.7564\n",
      "Epoch 163/300\n",
      "55620/55620 [==============================] - 47s 847us/step - loss: 0.9501 - accuracy: 0.7541\n",
      "Epoch 164/300\n",
      "55620/55620 [==============================] - 47s 841us/step - loss: 0.9383 - accuracy: 0.7589\n",
      "Epoch 165/300\n",
      "55620/55620 [==============================] - 47s 845us/step - loss: 0.9083 - accuracy: 0.7654\n",
      "Epoch 166/300\n",
      "55620/55620 [==============================] - 47s 842us/step - loss: 0.8907 - accuracy: 0.7709\n",
      "Epoch 167/300\n",
      "55620/55620 [==============================] - 47s 841us/step - loss: 0.8863 - accuracy: 0.7731\n",
      "Epoch 168/300\n",
      "55620/55620 [==============================] - 47s 842us/step - loss: 0.8752 - accuracy: 0.7706\n",
      "Epoch 169/300\n",
      "55620/55620 [==============================] - 47s 841us/step - loss: 0.8596 - accuracy: 0.7798\n",
      "Epoch 170/300\n",
      "55620/55620 [==============================] - 47s 843us/step - loss: 0.8518 - accuracy: 0.7805\n",
      "Epoch 171/300\n",
      "55620/55620 [==============================] - 47s 841us/step - loss: 0.8465 - accuracy: 0.7787\n",
      "Epoch 172/300\n",
      "55620/55620 [==============================] - 47s 841us/step - loss: 0.8351 - accuracy: 0.7834\n",
      "Epoch 173/300\n",
      "55620/55620 [==============================] - 47s 845us/step - loss: 0.8211 - accuracy: 0.7886\n",
      "Epoch 174/300\n",
      "55620/55620 [==============================] - 47s 843us/step - loss: 0.8029 - accuracy: 0.7928\n",
      "Epoch 175/300\n",
      "55620/55620 [==============================] - 47s 841us/step - loss: 0.8077 - accuracy: 0.7888\n",
      "Epoch 176/300\n",
      "55620/55620 [==============================] - 47s 843us/step - loss: 0.8118 - accuracy: 0.7883\n",
      "Epoch 177/300\n",
      "55620/55620 [==============================] - 48s 855us/step - loss: 0.7805 - accuracy: 0.7973\n",
      "Epoch 178/300\n",
      "55620/55620 [==============================] - 47s 847us/step - loss: 0.7619 - accuracy: 0.8016\n",
      "Epoch 179/300\n",
      "55620/55620 [==============================] - 47s 843us/step - loss: 0.7621 - accuracy: 0.8021\n",
      "Epoch 180/300\n",
      "55620/55620 [==============================] - 47s 841us/step - loss: 0.7567 - accuracy: 0.8038\n",
      "Epoch 181/300\n",
      "55620/55620 [==============================] - 47s 848us/step - loss: 0.7376 - accuracy: 0.8071\n",
      "Epoch 182/300\n",
      "55620/55620 [==============================] - 47s 843us/step - loss: 0.7205 - accuracy: 0.8132\n",
      "Epoch 183/300\n",
      "55620/55620 [==============================] - 47s 844us/step - loss: 0.7164 - accuracy: 0.8152\n",
      "Epoch 184/300\n",
      "55620/55620 [==============================] - 47s 841us/step - loss: 0.7131 - accuracy: 0.8151\n",
      "Epoch 185/300\n",
      "55620/55620 [==============================] - 47s 846us/step - loss: 0.6977 - accuracy: 0.8186\n",
      "Epoch 186/300\n",
      "55620/55620 [==============================] - 47s 843us/step - loss: 0.6881 - accuracy: 0.8225\n",
      "Epoch 187/300\n",
      "55620/55620 [==============================] - 47s 842us/step - loss: 0.6882 - accuracy: 0.8212\n",
      "Epoch 188/300\n",
      "55620/55620 [==============================] - 47s 844us/step - loss: 0.6855 - accuracy: 0.8215\n",
      "Epoch 189/300\n",
      "55620/55620 [==============================] - 47s 842us/step - loss: 0.6777 - accuracy: 0.8218\n",
      "Epoch 190/300\n",
      "55620/55620 [==============================] - 47s 843us/step - loss: 0.6387 - accuracy: 0.8348\n",
      "Epoch 191/300\n",
      "55620/55620 [==============================] - 47s 845us/step - loss: 0.6193 - accuracy: 0.8403\n",
      "Epoch 192/300\n",
      "55620/55620 [==============================] - 47s 844us/step - loss: 0.6250 - accuracy: 0.8370\n",
      "Epoch 193/300\n",
      "55620/55620 [==============================] - 47s 844us/step - loss: 0.6453 - accuracy: 0.8302\n",
      "Epoch 194/300\n",
      "55620/55620 [==============================] - 47s 844us/step - loss: 0.6210 - accuracy: 0.8370\n",
      "Epoch 195/300\n",
      "55620/55620 [==============================] - 47s 843us/step - loss: 0.6062 - accuracy: 0.8430\n",
      "Epoch 196/300\n",
      "55620/55620 [==============================] - 47s 852us/step - loss: 0.6177 - accuracy: 0.8371\n",
      "Epoch 197/300\n",
      "55620/55620 [==============================] - 47s 848us/step - loss: 0.5931 - accuracy: 0.8452\n",
      "Epoch 198/300\n",
      "55620/55620 [==============================] - 47s 847us/step - loss: 0.5781 - accuracy: 0.8494\n",
      "Epoch 199/300\n",
      "55620/55620 [==============================] - 47s 844us/step - loss: 0.5865 - accuracy: 0.8451\n",
      "Epoch 200/300\n",
      "55620/55620 [==============================] - 47s 845us/step - loss: 0.5711 - accuracy: 0.8520\n",
      "Epoch 201/300\n",
      "55620/55620 [==============================] - 47s 848us/step - loss: 0.5569 - accuracy: 0.8551\n",
      "Epoch 202/300\n",
      "55620/55620 [==============================] - 47s 845us/step - loss: 0.5554 - accuracy: 0.8558\n",
      "Epoch 203/300\n",
      "55620/55620 [==============================] - 47s 845us/step - loss: 0.5683 - accuracy: 0.8493\n",
      "Epoch 204/300\n",
      "55620/55620 [==============================] - 49s 876us/step - loss: 0.5483 - accuracy: 0.8560\n",
      "Epoch 205/300\n",
      "55620/55620 [==============================] - 47s 846us/step - loss: 0.5267 - accuracy: 0.8641\n",
      "Epoch 206/300\n",
      "55620/55620 [==============================] - 47s 845us/step - loss: 0.5064 - accuracy: 0.8700\n",
      "Epoch 207/300\n",
      "55620/55620 [==============================] - 47s 845us/step - loss: 0.5248 - accuracy: 0.8615\n",
      "Epoch 208/300\n",
      "55620/55620 [==============================] - 47s 845us/step - loss: 0.5232 - accuracy: 0.8622\n",
      "Epoch 209/300\n",
      "55620/55620 [==============================] - 47s 846us/step - loss: 0.4817 - accuracy: 0.8766\n",
      "Epoch 210/300\n",
      "55620/55620 [==============================] - 46s 835us/step - loss: 0.5149 - accuracy: 0.8630\n",
      "Epoch 211/300\n",
      "55620/55620 [==============================] - 46s 836us/step - loss: 0.5018 - accuracy: 0.8688\n",
      "Epoch 212/300\n",
      "55620/55620 [==============================] - 46s 833us/step - loss: 0.4688 - accuracy: 0.8786\n",
      "Epoch 213/300\n",
      "55620/55620 [==============================] - 46s 834us/step - loss: 0.4604 - accuracy: 0.8816\n",
      "Epoch 214/300\n",
      "55620/55620 [==============================] - 46s 835us/step - loss: 0.4649 - accuracy: 0.8786\n",
      "Epoch 215/300\n",
      "55620/55620 [==============================] - 47s 843us/step - loss: 0.4970 - accuracy: 0.8673\n",
      "Epoch 216/300\n",
      "55620/55620 [==============================] - 47s 838us/step - loss: 0.5091 - accuracy: 0.8629\n",
      "Epoch 217/300\n",
      "55620/55620 [==============================] - 47s 836us/step - loss: 0.4708 - accuracy: 0.8748\n",
      "Epoch 218/300\n",
      "55620/55620 [==============================] - 46s 834us/step - loss: 0.4398 - accuracy: 0.8871\n",
      "Epoch 219/300\n",
      "55620/55620 [==============================] - 47s 838us/step - loss: 0.4278 - accuracy: 0.8912\n",
      "Epoch 220/300\n",
      "55620/55620 [==============================] - 46s 834us/step - loss: 0.4083 - accuracy: 0.8959\n",
      "Epoch 221/300\n",
      "55620/55620 [==============================] - 46s 834us/step - loss: 0.4232 - accuracy: 0.8903\n",
      "Epoch 222/300\n",
      "55620/55620 [==============================] - 46s 835us/step - loss: 0.4333 - accuracy: 0.8869\n",
      "Epoch 223/300\n",
      "55620/55620 [==============================] - 47s 839us/step - loss: 0.4555 - accuracy: 0.8775\n",
      "Epoch 224/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55620/55620 [==============================] - 46s 834us/step - loss: 0.4613 - accuracy: 0.8748\n",
      "Epoch 225/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.4029 - accuracy: 0.8964\n",
      "Epoch 226/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.3964 - accuracy: 0.8969\n",
      "Epoch 227/300\n",
      "55620/55620 [==============================] - 46s 833us/step - loss: 0.3641 - accuracy: 0.9087\n",
      "Epoch 228/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.3582 - accuracy: 0.9096\n",
      "Epoch 229/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.3827 - accuracy: 0.9007\n",
      "Epoch 230/300\n",
      "55620/55620 [==============================] - 47s 840us/step - loss: 0.4371 - accuracy: 0.8792\n",
      "Epoch 231/300\n",
      "55620/55620 [==============================] - 46s 834us/step - loss: 0.4290 - accuracy: 0.8815\n",
      "Epoch 232/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.3754 - accuracy: 0.9017\n",
      "Epoch 233/300\n",
      "55620/55620 [==============================] - 46s 834us/step - loss: 0.3288 - accuracy: 0.9181\n",
      "Epoch 234/300\n",
      "55620/55620 [==============================] - 46s 829us/step - loss: 0.3214 - accuracy: 0.9200\n",
      "Epoch 235/300\n",
      "55620/55620 [==============================] - 47s 839us/step - loss: 0.3581 - accuracy: 0.9057\n",
      "Epoch 236/300\n",
      "55620/55620 [==============================] - 46s 833us/step - loss: 0.3968 - accuracy: 0.8923\n",
      "Epoch 237/300\n",
      "55620/55620 [==============================] - 46s 831us/step - loss: 0.4197 - accuracy: 0.8825\n",
      "Epoch 238/300\n",
      "55620/55620 [==============================] - 46s 834us/step - loss: 0.3441 - accuracy: 0.9093\n",
      "Epoch 239/300\n",
      "55620/55620 [==============================] - 46s 831us/step - loss: 0.3099 - accuracy: 0.9212\n",
      "Epoch 240/300\n",
      "55620/55620 [==============================] - 46s 834us/step - loss: 0.2942 - accuracy: 0.9268\n",
      "Epoch 241/300\n",
      "55620/55620 [==============================] - 46s 830us/step - loss: 0.2899 - accuracy: 0.9276\n",
      "Epoch 242/300\n",
      "55620/55620 [==============================] - 46s 835us/step - loss: 0.3492 - accuracy: 0.9054\n",
      "Epoch 243/300\n",
      "55620/55620 [==============================] - 46s 831us/step - loss: 0.4365 - accuracy: 0.8752\n",
      "Epoch 244/300\n",
      "55620/55620 [==============================] - 46s 831us/step - loss: 0.3586 - accuracy: 0.9030\n",
      "Epoch 245/300\n",
      "55620/55620 [==============================] - 46s 833us/step - loss: 0.2795 - accuracy: 0.9313\n",
      "Epoch 246/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.2424 - accuracy: 0.9429\n",
      "Epoch 247/300\n",
      "55620/55620 [==============================] - 46s 833us/step - loss: 0.2718 - accuracy: 0.9319\n",
      "Epoch 248/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.4020 - accuracy: 0.8862\n",
      "Epoch 249/300\n",
      "55620/55620 [==============================] - 46s 833us/step - loss: 0.3747 - accuracy: 0.8949\n",
      "Epoch 250/300\n",
      "55620/55620 [==============================] - 46s 835us/step - loss: 0.2864 - accuracy: 0.9262\n",
      "Epoch 251/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.2658 - accuracy: 0.9335\n",
      "Epoch 252/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.2596 - accuracy: 0.9352\n",
      "Epoch 253/300\n",
      "55620/55620 [==============================] - 46s 833us/step - loss: 0.2692 - accuracy: 0.9317\n",
      "Epoch 254/300\n",
      "55620/55620 [==============================] - 47s 841us/step - loss: 0.2967 - accuracy: 0.9206\n",
      "Epoch 255/300\n",
      "55620/55620 [==============================] - 47s 839us/step - loss: 0.3382 - accuracy: 0.9054\n",
      "Epoch 256/300\n",
      "55620/55620 [==============================] - 46s 831us/step - loss: 0.3296 - accuracy: 0.9088\n",
      "Epoch 257/300\n",
      "55620/55620 [==============================] - 46s 833us/step - loss: 0.2524 - accuracy: 0.9361\n",
      "Epoch 258/300\n",
      "55620/55620 [==============================] - 46s 835us/step - loss: 0.2329 - accuracy: 0.9428\n",
      "Epoch 259/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.2000 - accuracy: 0.9531\n",
      "Epoch 260/300\n",
      "55620/55620 [==============================] - 46s 833us/step - loss: 0.3493 - accuracy: 0.9031\n",
      "Epoch 261/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.3947 - accuracy: 0.8868\n",
      "Epoch 262/300\n",
      "55620/55620 [==============================] - 46s 834us/step - loss: 0.2874 - accuracy: 0.9227\n",
      "Epoch 263/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.2239 - accuracy: 0.9452\n",
      "Epoch 264/300\n",
      "55620/55620 [==============================] - 46s 831us/step - loss: 0.1856 - accuracy: 0.9589\n",
      "Epoch 265/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.2188 - accuracy: 0.9463\n",
      "Epoch 266/300\n",
      "55620/55620 [==============================] - 46s 834us/step - loss: 0.3241 - accuracy: 0.9089\n",
      "Epoch 267/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.3630 - accuracy: 0.8967\n",
      "Epoch 268/300\n",
      "55620/55620 [==============================] - 46s 835us/step - loss: 0.2738 - accuracy: 0.9264\n",
      "Epoch 269/300\n",
      "55620/55620 [==============================] - 46s 834us/step - loss: 0.2045 - accuracy: 0.9502\n",
      "Epoch 270/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.1753 - accuracy: 0.9598\n",
      "Epoch 271/300\n",
      "55620/55620 [==============================] - 46s 833us/step - loss: 0.1967 - accuracy: 0.9521\n",
      "Epoch 272/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.3209 - accuracy: 0.9100\n",
      "Epoch 273/300\n",
      "55620/55620 [==============================] - 46s 833us/step - loss: 0.3182 - accuracy: 0.9086\n",
      "Epoch 274/300\n",
      "55620/55620 [==============================] - 47s 840us/step - loss: 0.2256 - accuracy: 0.9411\n",
      "Epoch 275/300\n",
      "55620/55620 [==============================] - 46s 834us/step - loss: 0.1753 - accuracy: 0.9589\n",
      "Epoch 276/300\n",
      "55620/55620 [==============================] - 46s 833us/step - loss: 0.1801 - accuracy: 0.9568\n",
      "Epoch 277/300\n",
      "55620/55620 [==============================] - 46s 833us/step - loss: 0.2375 - accuracy: 0.9354\n",
      "Epoch 278/300\n",
      "55620/55620 [==============================] - 46s 835us/step - loss: 0.2978 - accuracy: 0.9155\n",
      "Epoch 279/300\n",
      "55620/55620 [==============================] - 46s 833us/step - loss: 0.2628 - accuracy: 0.9274\n",
      "Epoch 280/300\n",
      "55620/55620 [==============================] - 46s 831us/step - loss: 0.2015 - accuracy: 0.9476\n",
      "Epoch 281/300\n",
      "55620/55620 [==============================] - 47s 847us/step - loss: 0.1691 - accuracy: 0.9601\n",
      "Epoch 282/300\n",
      "55620/55620 [==============================] - 47s 837us/step - loss: 0.2026 - accuracy: 0.9485\n",
      "Epoch 283/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.3036 - accuracy: 0.9122\n",
      "Epoch 284/300\n",
      "55620/55620 [==============================] - 46s 835us/step - loss: 0.2654 - accuracy: 0.9249\n",
      "Epoch 285/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.1890 - accuracy: 0.9516\n",
      "Epoch 286/300\n",
      "55620/55620 [==============================] - 47s 837us/step - loss: 0.1819 - accuracy: 0.9544\n",
      "Epoch 287/300\n",
      "55620/55620 [==============================] - 46s 833us/step - loss: 0.1753 - accuracy: 0.9565\n",
      "Epoch 288/300\n",
      "55620/55620 [==============================] - 47s 841us/step - loss: 0.1829 - accuracy: 0.9538\n",
      "Epoch 289/300\n",
      "55620/55620 [==============================] - 46s 834us/step - loss: 0.2441 - accuracy: 0.9318\n",
      "Epoch 290/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.2572 - accuracy: 0.9281\n",
      "Epoch 291/300\n",
      "55620/55620 [==============================] - 46s 833us/step - loss: 0.2288 - accuracy: 0.9374\n",
      "Epoch 292/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.1913 - accuracy: 0.9502\n",
      "Epoch 293/300\n",
      "55620/55620 [==============================] - 47s 843us/step - loss: 0.1373 - accuracy: 0.9697\n",
      "Epoch 294/300\n",
      "55620/55620 [==============================] - 47s 838us/step - loss: 0.1884 - accuracy: 0.9508\n",
      "Epoch 295/300\n",
      "55620/55620 [==============================] - 46s 834us/step - loss: 0.2224 - accuracy: 0.9373\n",
      "Epoch 296/300\n",
      "55620/55620 [==============================] - 46s 835us/step - loss: 0.2299 - accuracy: 0.9348\n",
      "Epoch 297/300\n",
      "55620/55620 [==============================] - 46s 835us/step - loss: 0.2375 - accuracy: 0.9332\n",
      "Epoch 298/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55620/55620 [==============================] - 46s 831us/step - loss: 0.2030 - accuracy: 0.9449\n",
      "Epoch 299/300\n",
      "55620/55620 [==============================] - 46s 832us/step - loss: 0.1586 - accuracy: 0.9593\n",
      "Epoch 300/300\n",
      "55620/55620 [==============================] - 46s 831us/step - loss: 0.1151 - accuracy: 0.9757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x20265eb9588>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##fit the model\n",
    "model.fit(X, y, batch_size=128, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7446f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "##save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "24e9e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load saved model and tokenzer\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model.h5')\n",
    "\n",
    "with open('tokenizer', 'rb') as file:\n",
    "    tokenizer = load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b5b4268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##generating sequences using model and tokenizer\n",
    "\n",
    "def generate_sequences(model, tokenizer, seq_len, seed_text, num_words):\n",
    "    \n",
    "    output_text = [] ##list to collect output\n",
    "    \n",
    "    input_text = seed_text ##take 15 words as user input to initialize text generation\n",
    "    \n",
    "    for i in range(num_words):\n",
    "        \n",
    "        encoded_input = tokenizer.texts_to_sequences([input_text])[0] ##encode input using tokenizer vocabulary\n",
    "        \n",
    "        padded_input = pad_sequences([encoded_input], maxlen=seq_len, truncating='pre', padding='pre') ##pad sequence\n",
    "        \n",
    "        word_index = model.predict_classes(padded_input)[0] ##getting the index of the next word in tokenizer.index_word\n",
    "        \n",
    "        word = tokenizer.index_word[word_index] ##word from tokenizer vocabulary\n",
    "        \n",
    "        input_text += ' ' + word ##add word to next iteration input\n",
    "        \n",
    "        output_text.append(word)\n",
    "    \n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "59ea5cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "##SOME EXAMPLES OF TEXT GENERATION USING OUR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5d0166dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in the aid of imagination in america and if a point of growth and and even eﬀort ⁵ and wildly'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text = \"the most important thing to understand about personal finance it its pertinence in modern human life\"\n",
    "generate_sequences(model=model, tokenizer=tokenizer, seq_len=seq_len, seed_text=seed_text, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0a0416c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'our tendency and their goals i sleep out a day what people book did n’t be rational at the number of my own head i expect more the ones on the power of reasonable going this do you might have mostly incomes ” as i wiped out paying not to to be understood is attributable to those who things are found the opposite on productivity now you are in the way anyone expected you do n’t need to expect them better people crime wealth their decisions is so taking that magic in a mountaineering chapter as much progress buﬀett else'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text = \"the reason why many people stay poor for a majority of their years is because they are yet to understand\"\n",
    "generate_sequences(model=model, tokenizer=tokenizer, seq_len=seq_len, seed_text=seed_text, num_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4c7d8bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'initial some opportunities were the best price available to be'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text ='no seed text'\n",
    "generate_sequences(model=model, tokenizer=tokenizer, seq_len=seq_len, seed_text=seed_text, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845758d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
